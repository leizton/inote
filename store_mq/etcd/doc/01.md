# 01 basic
- etcd是一个分布式kv存储系统,用于共享配置和服务发现,底层通过raft实现一致性
- 记录key的每个历史版本值
- 维护一个字段序的B树索引
- ./bin/etcd
    --name $node3  ;当前节点名称
    --listen-peer-urls http://127.0.0.1:2383  ;当前节点bind这个url,使其他节点可以和自己通信
    --initial-advertise-peer-urls http://127.0.0.1:2383  ;same with listen-peer-urls
    --listen-client-urls http://127.0.0.1:2393  ;当前节点bind这个url, 好让client访问自己
    --advertise-client-urls http://127.0.0.1:2393  ;same with listen-client-urls
    --initial-cluster-token myetcd1  ;所属集群的标识
    --initial-cluster $node1=http://127.0.0.1:2381,$node2=http://127.0.0.1:2382,$node3=http://127.0.0.1:2383  ;集群中所有节点的initial-advertise-peer-urls集合
    --initial-cluster-state new
- etcd启动方式: 静态配置发现, 服务发现, DNS发现


# 02 raft
- 在程序不会崩溃(代码bug,服务器掉电等),网络可靠且无延迟的理想情况下,容易保持集群中各节点间状态一致
- 一致性协议(如paxos,raft)保证集群中超过半数以上节点可用时,集群能给出正确的结果
- Paxos在理论上证明是正确的,但难以理解和实现,其实现都做了大的改动而未经证明
## raft leader选举
- 2个超时时间: heartbeat-timeout election-timeout
- 节点有3种状态(角色): leader, follower, candidate
- node初始状态是follower
  follower: 如果已有leader,当leader的心跳超时,则启动election计数器; 如果没有leader,则直接启动election-timer;
    当election-timer超时,变成candidate,以发起新一轮投票;
    收到投票请求时,若未透出票,则投票,否则拒绝, 投票后清零election-timer防止同一时间出现多个candidate;
  candidate: 先向自己投1票, 把任期term号增1, 并标记这一任期的选票投给了自己;
    再向其他node发出投票请求, 并清零election-timer;
    当收到超过半数的投票(包括自己的一票)时变成leader, 并向follower发心跳防止follower的election-timer超时,
    因此heartbeat-timeout应小于election-timeout, 防止出现多个candidate;
  leader: 接收client的请求; 和follower维持心跳, follower收到心跳会重置election-timer;
- 每个node记录当前任期号和自己的投票结果
- 选举时, 如果candidate数量大于1, 可能导致这一轮选举失败.
  因此每个candidate的election-timeout是在一个区间上的随机值, 使得同时有多个candidate概率变小, 是活锁
- raft要求: 广播时间 << election-timeout << 平均故障间隔时间.
  广播时间指集群中任意节点通信的往返时间(round trip time, RTT)
## 日志复制
- leader对于每个client的update请求都写日志记录, 并把record通过发消息同步给follower, 超过半数follower响应同步成功时认为该记录是committed;
  committed的record会被leader应用到自身的状态机中, 同时leader会通知follower应用该record; 最后响应client
- 每个node维护commitIdx和lastAppliedIdx. commitIdx是当前节点最大的日志索引值, lastAppliedIdx是最后一条被应用到状态机的record的日志索引值.
  leader维护了每个follower的matchIdx和nextIdx, matchIdx是follower响应leader同步成功的位置, nextIdx是leader下次向follower发日志的起始位置,
  由于采用批量发送, nextIdx和matchIdx间的差值 <= batchNum
- leader更替后, new_leader并不知道old_leader的matchIdx和nextIdx, 因此matchIdx都置0, nextIdx置成自己的commitIdx;
  当某些follower同步比new_leader慢, 则会响应追加日志失败, new_leader减少其nextIdx
- 选举时, follower会拒绝投票给commitIdx小于自己的candidate, 以保证不丢失record
- client如果连到某个follower时, follower会返回当前leader给client.
  另一种方案是follower作proxy, follower把client的请求转发给leader
- 集群中只有一个leader处理请求, 所以kafka的topic分partition可以提高集群整体负载能力
## 网络分区的处理
- 假设集群中有5个节点A~E, A是term=1的leader, 此时{A,B}和{C,D,E}形成两个网络分区.
  由于C,D,E收不到A的心跳, 所以至少有一个节点会发生election-timeout, 假设是E.
  E变成candidate发起选举, 收到C,D投票后总票数是3, 超过半数而变成term=2的leader.
  网络恢复后, A发心跳给CDE后会知道term已变成2, 然后切换成follower.
  发生分区时A会堆积未提交的日志, 这些record在网络恢复后被回滚, 然后A从E同步新的record
- 如果{A,B,C}和{D,E}形成分区, {D,E}分区内的投票数不会超过半数.
  为了防止不断的选举失败导致term不断增加, 在election超时和发起选举之间引入preVote状态.
  preVote状态里如果能与半数以上的节点通信, 则进入选举状态
## 日志压缩与快照
- 节点重启后需要回访日志, 如果日志过多则回放很慢
- 定期生成快照, 删除快照前的日志. 快照: 系统某个时间点的一个瞬时状态.
  形成快照时, 系统每个独立部分的瞬时时间点无需相同, 即无需暂停整个系统
- 如果快照很大, 则follower自己生成快照; 如果快照小, 则可让leader同步自己的快照给follower
## linearizable语义
- client的每次请求必须加超时, 防止网络或节点宕机等原因导致client请求永久卡住.
  req有超时, 则必然有重试. 为了在发生重试时, client能分辨出resp对应哪次req, client必须为每个req设置opaque
- 对于重试的写请求, server可对client维护一个session, 防止重复执行写请求
- linearizble与atomic consistency同义, 对单个对象的操作顺序(读写请求处理顺序)是按时间排序的.
  也是CAP原则中的C