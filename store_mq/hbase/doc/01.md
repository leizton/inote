# 一致性程度, 由强到弱
严格一致: 数据变化具有原子性
顺序一致: 每个客户端看到的数据变化顺序与它们的操作顺序一致
因果一致: 客户端以因果顺序观察到数据的变化
最终一致: 在没有数据更新的时间段中, 系统通过广播保证副本之间的数据一致
弱一致:   不同客户端看到的数据顺序可能不一样

# CAP定理
一个分布式系统只能同时实现一致性(C), 可用性(A), 分区容错性(P)中的2个

# 阻抗匹配
给问题找到一个理想的解决方案, 从众多可用方案中选出最适合的

# 反范式化, 复制
例如把数据复制到多张表中, 这样读取时涉及的表数目被减少

# 表 行 列 单元格
"table"  "row"  "family:qualifier"  "cell"
一张表有许多行, 每行的列族在1~50, 每个列族的qualifier在1~10^6
每个qualifier有多个版本, 一个版本的值就是cell的值, 因此cell有timestamp属性
一个列族的所有列存储在同一个底层文件"HFile"里

# 键值映射结构
"(table row family qualifier timestamp) --> value"
行数据的存取是原子的, 可以并发地读写一行的任意数目的列
跨行和跨表事务不支持

# region
HBase-中扩展和负载均衡的基本单元是region
每个region最多只能由一台服务器加载, 一台服务器可以加载多个region
region是分布式存储的最小单元
region的最佳大小是1~2GB
每张表一开始只有1个region, 表大了后根据rowkey拆分, 拆分策略如
  ConstantSizeRegionSplitPolicy(超过一定大小), KeyPrefixRegionSplitPolicy(相同前缀的在同一个region)
一个region由一个或多个store组成, 每个store保存一个column_family(列族)
store包含一个memStore(memory)和多个storeFile, storeFile保存在一个hdfs文件
region状态: unassign assign split merge

# HFile的存储-删除-合并小文件
rowKey的顺序是按照byte[]逐字节比较, 所以"key-22"在"key-3"前面
HFile-存储经过排序的键值, 文件内部由连续的"块(默认64KB)"组成, "块索引信息"在文件尾部,
    打开HFile时索引信息优先加载到内存. 在内存中对块索引用二分查找, 确定可能包含所
    查键的块后, 再从磁盘读入该块
因为文件是连续存储, 删除键值时使用"删除标记", 而非真实地删除
2种文件合并类型:
    "minor": 把多个小文件重写成数量较少的大文件, 对这些小文件的键值做多路归并
    "major": 把一个列族的多个HFile重写成一个HFile, 此时需扫描所有键值对,
             重写时略过有删除标记的数据, 和超过版本号限制生存时间到期的数据

# HBase存储能力
数十亿行 × 数百万列 × 数千个版本 = TB/PB-级数据

# 预写日志 WAL
每次更新数据时, 会先记录在"提交日志"中

# example
Configuration conf = HBaseConfiguration.create();
HTable table = new HTable(conf, "tableName");
Put put = new Put(Bytes.toBytes("row1")); // 每行对应一个Put
put.add(Bytes.toBytes("column1"), Bytes.toBytes("qualifier1"), Bytes.toBytes("value1"));
put.add(Bytes.toBytes("column2"), Bytes.toBytes("qualifier2"), Bytes.toBytes("value2"));
table.put(put);

# B+ LSM 树
B+树的范围扫描高效，因为叶节点相连且按主键有序。
B+树可以索引到页表。
当页表超过容量限制时，会被拆分成2个新页表，同时父节点指向新的2个页表。
B+树让逻辑上相近的页表在磁盘上的物理位置也相近，这样大概率下提高读磁盘速度。
在没有太多修改时，B+树性能很好，修改会引起高代价的优化查询操作。
删除时，仅存储删除标记，当页被重写时有删除标记的键被丢弃。